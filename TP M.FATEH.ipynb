{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165424f-3726-4cdf-ae42-1536461831b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating AMHCD data loading and preprocessing...\n",
      "Simulated data shape: X=(25740, 1024), y=(25740,)\n",
      "Applying data augmentation (simulated)...\n",
      "Augmented data shape: X_augmented=(51480, 1024), y_augmented=(51480,)\n",
      "\n",
      "Starting 5-fold cross-validation...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 1/50: Train Loss = 3.5026, Train Accuracy = 0.0316\n",
      "  Validation Loss = 3.4964, Validation Accuracy = 0.0324\n",
      "Epoch 2/50: Train Loss = 3.4981, Train Accuracy = 0.0320\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0286\n",
      "Epoch 3/50: Train Loss = 3.4982, Train Accuracy = 0.0309\n",
      "  Validation Loss = 3.4968, Validation Accuracy = 0.0341\n",
      "Epoch 4/50: Train Loss = 3.4982, Train Accuracy = 0.0323\n",
      "  Validation Loss = 3.4958, Validation Accuracy = 0.0334\n",
      "Epoch 5/50: Train Loss = 3.4980, Train Accuracy = 0.0327\n",
      "  Validation Loss = 3.4986, Validation Accuracy = 0.0302\n",
      "Epoch 6/50: Train Loss = 3.4981, Train Accuracy = 0.0294\n",
      "  Validation Loss = 3.4975, Validation Accuracy = 0.0287\n",
      "Epoch 7/50: Train Loss = 3.4983, Train Accuracy = 0.0306\n",
      "  Validation Loss = 3.4960, Validation Accuracy = 0.0341\n",
      "Epoch 8/50: Train Loss = 3.4984, Train Accuracy = 0.0305\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0341\n",
      "Epoch 9/50: Train Loss = 3.4981, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4965, Validation Accuracy = 0.0334\n",
      "Epoch 10/50: Train Loss = 3.4981, Train Accuracy = 0.0309\n",
      "  Validation Loss = 3.4965, Validation Accuracy = 0.0303\n",
      "Epoch 11/50: Train Loss = 3.4983, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4970, Validation Accuracy = 0.0334\n",
      "Epoch 12/50: Train Loss = 3.4981, Train Accuracy = 0.0321\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0327\n",
      "Epoch 13/50: Train Loss = 3.4981, Train Accuracy = 0.0298\n",
      "  Validation Loss = 3.4963, Validation Accuracy = 0.0341\n",
      "Epoch 14/50: Train Loss = 3.4981, Train Accuracy = 0.0302\n",
      "  Validation Loss = 3.4970, Validation Accuracy = 0.0328\n",
      "Epoch 15/50: Train Loss = 3.4978, Train Accuracy = 0.0300\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0341\n",
      "Epoch 16/50: Train Loss = 3.4983, Train Accuracy = 0.0308\n",
      "  Validation Loss = 3.4960, Validation Accuracy = 0.0320\n",
      "Epoch 17/50: Train Loss = 3.4980, Train Accuracy = 0.0312\n",
      "  Validation Loss = 3.4970, Validation Accuracy = 0.0324\n",
      "Epoch 18/50: Train Loss = 3.4982, Train Accuracy = 0.0287\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0304\n",
      "Epoch 19/50: Train Loss = 3.4980, Train Accuracy = 0.0305\n",
      "  Validation Loss = 3.4983, Validation Accuracy = 0.0332\n",
      "Epoch 20/50: Train Loss = 3.4982, Train Accuracy = 0.0302\n",
      "  Validation Loss = 3.4962, Validation Accuracy = 0.0332\n",
      "Epoch 21/50: Train Loss = 3.4981, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4972, Validation Accuracy = 0.0327\n",
      "Epoch 22/50: Train Loss = 3.4983, Train Accuracy = 0.0288\n",
      "  Validation Loss = 3.4980, Validation Accuracy = 0.0328\n",
      "Epoch 23/50: Train Loss = 3.4982, Train Accuracy = 0.0315\n",
      "  Validation Loss = 3.4964, Validation Accuracy = 0.0302\n",
      "Epoch 24/50: Train Loss = 3.4982, Train Accuracy = 0.0301\n",
      "  Validation Loss = 3.4982, Validation Accuracy = 0.0334\n",
      "Epoch 25/50: Train Loss = 3.4978, Train Accuracy = 0.0329\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0302\n",
      "Epoch 26/50: Train Loss = 3.4983, Train Accuracy = 0.0299\n",
      "  Validation Loss = 3.4969, Validation Accuracy = 0.0286\n",
      "Epoch 27/50: Train Loss = 3.4982, Train Accuracy = 0.0303\n",
      "  Validation Loss = 3.4968, Validation Accuracy = 0.0334\n",
      "Epoch 28/50: Train Loss = 3.4980, Train Accuracy = 0.0312\n",
      "  Validation Loss = 3.4973, Validation Accuracy = 0.0302\n",
      "Epoch 29/50: Train Loss = 3.4980, Train Accuracy = 0.0299\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0270\n",
      "Epoch 30/50: Train Loss = 3.4981, Train Accuracy = 0.0306\n",
      "  Validation Loss = 3.4969, Validation Accuracy = 0.0320\n",
      "Epoch 31/50: Train Loss = 3.4981, Train Accuracy = 0.0316\n",
      "  Validation Loss = 3.4963, Validation Accuracy = 0.0302\n",
      "Epoch 32/50: Train Loss = 3.4980, Train Accuracy = 0.0294\n",
      "  Validation Loss = 3.4977, Validation Accuracy = 0.0324\n",
      "Epoch 33/50: Train Loss = 3.4984, Train Accuracy = 0.0289\n",
      "  Validation Loss = 3.4964, Validation Accuracy = 0.0327\n",
      "Epoch 34/50: Train Loss = 3.4982, Train Accuracy = 0.0307\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0302\n",
      "Epoch 35/50: Train Loss = 3.4981, Train Accuracy = 0.0300\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0287\n",
      "Epoch 36/50: Train Loss = 3.4982, Train Accuracy = 0.0300\n",
      "  Validation Loss = 3.4965, Validation Accuracy = 0.0324\n",
      "Epoch 37/50: Train Loss = 3.4981, Train Accuracy = 0.0296\n",
      "  Validation Loss = 3.4973, Validation Accuracy = 0.0287\n",
      "Epoch 38/50: Train Loss = 3.4981, Train Accuracy = 0.0312\n",
      "  Validation Loss = 3.4969, Validation Accuracy = 0.0303\n",
      "Epoch 39/50: Train Loss = 3.4980, Train Accuracy = 0.0297\n",
      "  Validation Loss = 3.4962, Validation Accuracy = 0.0341\n",
      "Epoch 40/50: Train Loss = 3.4978, Train Accuracy = 0.0305\n",
      "  Validation Loss = 3.4987, Validation Accuracy = 0.0287\n",
      "Epoch 41/50: Train Loss = 3.4983, Train Accuracy = 0.0297\n",
      "  Validation Loss = 3.4981, Validation Accuracy = 0.0287\n",
      "Epoch 42/50: Train Loss = 3.4982, Train Accuracy = 0.0288\n",
      "  Validation Loss = 3.4965, Validation Accuracy = 0.0324\n",
      "Epoch 43/50: Train Loss = 3.4980, Train Accuracy = 0.0311\n",
      "  Validation Loss = 3.4965, Validation Accuracy = 0.0327\n",
      "Epoch 44/50: Train Loss = 3.4981, Train Accuracy = 0.0309\n",
      "  Validation Loss = 3.4966, Validation Accuracy = 0.0341\n",
      "Epoch 45/50: Train Loss = 3.4983, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4960, Validation Accuracy = 0.0320\n",
      "Epoch 46/50: Train Loss = 3.4980, Train Accuracy = 0.0318\n",
      "  Validation Loss = 3.4957, Validation Accuracy = 0.0324\n",
      "Epoch 47/50: Train Loss = 3.4982, Train Accuracy = 0.0307\n",
      "  Validation Loss = 3.4970, Validation Accuracy = 0.0328\n",
      "Epoch 48/50: Train Loss = 3.4982, Train Accuracy = 0.0292\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0302\n",
      "Epoch 49/50: Train Loss = 3.4978, Train Accuracy = 0.0321\n",
      "  Validation Loss = 3.4981, Validation Accuracy = 0.0270\n",
      "Epoch 50/50: Train Loss = 3.4981, Train Accuracy = 0.0300\n",
      "  Validation Loss = 3.4958, Validation Accuracy = 0.0304\n",
      "Fold 1 Final Validation Loss: 3.4958\n",
      "Fold 1 Final Validation Accuracy: 0.0304\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Epoch 1/50: Train Loss = 3.5254, Train Accuracy = 0.0303\n",
      "  Validation Loss = 3.4982, Validation Accuracy = 0.0305\n",
      "Epoch 2/50: Train Loss = 3.4980, Train Accuracy = 0.0293\n",
      "  Validation Loss = 3.4975, Validation Accuracy = 0.0287\n",
      "Epoch 3/50: Train Loss = 3.4982, Train Accuracy = 0.0298\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0309\n",
      "Epoch 4/50: Train Loss = 3.4976, Train Accuracy = 0.0302\n",
      "  Validation Loss = 3.4993, Validation Accuracy = 0.0277\n",
      "Epoch 5/50: Train Loss = 3.4980, Train Accuracy = 0.0308\n",
      "  Validation Loss = 3.4981, Validation Accuracy = 0.0325\n",
      "Epoch 6/50: Train Loss = 3.4997, Train Accuracy = 0.0295\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0309\n",
      "Epoch 7/50: Train Loss = 3.4981, Train Accuracy = 0.0302\n",
      "  Validation Loss = 3.4977, Validation Accuracy = 0.0333\n",
      "Epoch 8/50: Train Loss = 3.4980, Train Accuracy = 0.0305\n",
      "  Validation Loss = 3.4975, Validation Accuracy = 0.0325\n",
      "Epoch 9/50: Train Loss = 3.4978, Train Accuracy = 0.0294\n",
      "  Validation Loss = 3.4975, Validation Accuracy = 0.0325\n",
      "Epoch 10/50: Train Loss = 3.4978, Train Accuracy = 0.0287\n",
      "  Validation Loss = 3.4979, Validation Accuracy = 0.0287\n",
      "Epoch 11/50: Train Loss = 3.4979, Train Accuracy = 0.0317\n",
      "  Validation Loss = 3.4973, Validation Accuracy = 0.0305\n",
      "Epoch 12/50: Train Loss = 3.4976, Train Accuracy = 0.0311\n",
      "  Validation Loss = 3.4978, Validation Accuracy = 0.0309\n",
      "Epoch 13/50: Train Loss = 3.4979, Train Accuracy = 0.0306\n",
      "  Validation Loss = 3.4988, Validation Accuracy = 0.0305\n",
      "Epoch 14/50: Train Loss = 3.4980, Train Accuracy = 0.0301\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0325\n",
      "Epoch 15/50: Train Loss = 3.4978, Train Accuracy = 0.0314\n",
      "  Validation Loss = 3.4986, Validation Accuracy = 0.0296\n",
      "Epoch 16/50: Train Loss = 3.4980, Train Accuracy = 0.0301\n",
      "  Validation Loss = 3.4989, Validation Accuracy = 0.0305\n",
      "Epoch 17/50: Train Loss = 3.4982, Train Accuracy = 0.0307\n",
      "  Validation Loss = 3.4977, Validation Accuracy = 0.0325\n",
      "Epoch 18/50: Train Loss = 3.4980, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4982, Validation Accuracy = 0.0325\n",
      "Epoch 19/50: Train Loss = 3.4977, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0301\n",
      "Epoch 20/50: Train Loss = 3.4982, Train Accuracy = 0.0316\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0287\n",
      "Epoch 21/50: Train Loss = 3.4980, Train Accuracy = 0.0303\n",
      "  Validation Loss = 3.4983, Validation Accuracy = 0.0305\n",
      "Epoch 22/50: Train Loss = 3.4978, Train Accuracy = 0.0308\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0287\n",
      "Epoch 23/50: Train Loss = 3.4979, Train Accuracy = 0.0307\n",
      "  Validation Loss = 3.4979, Validation Accuracy = 0.0283\n",
      "Epoch 24/50: Train Loss = 3.4981, Train Accuracy = 0.0313\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0329\n",
      "Epoch 25/50: Train Loss = 3.4978, Train Accuracy = 0.0318\n",
      "  Validation Loss = 3.4988, Validation Accuracy = 0.0325\n",
      "Epoch 26/50: Train Loss = 3.4980, Train Accuracy = 0.0317\n",
      "  Validation Loss = 3.4982, Validation Accuracy = 0.0329\n",
      "Epoch 27/50: Train Loss = 3.4976, Train Accuracy = 0.0328\n",
      "  Validation Loss = 3.4976, Validation Accuracy = 0.0304\n",
      "Epoch 28/50: Train Loss = 3.4980, Train Accuracy = 0.0292\n",
      "  Validation Loss = 3.4990, Validation Accuracy = 0.0287\n",
      "Epoch 29/50: Train Loss = 3.4978, Train Accuracy = 0.0309\n",
      "  Validation Loss = 3.4994, Validation Accuracy = 0.0329\n",
      "Epoch 30/50: Train Loss = 3.4979, Train Accuracy = 0.0307\n",
      "  Validation Loss = 3.4977, Validation Accuracy = 0.0329\n",
      "Epoch 31/50: Train Loss = 3.4978, Train Accuracy = 0.0300\n",
      "  Validation Loss = 3.4983, Validation Accuracy = 0.0308\n",
      "Epoch 32/50: Train Loss = 3.4979, Train Accuracy = 0.0305\n",
      "  Validation Loss = 3.4982, Validation Accuracy = 0.0287\n",
      "Epoch 33/50: Train Loss = 3.4979, Train Accuracy = 0.0312\n",
      "  Validation Loss = 3.4977, Validation Accuracy = 0.0329\n",
      "Epoch 34/50: Train Loss = 3.4979, Train Accuracy = 0.0306\n",
      "  Validation Loss = 3.4979, Validation Accuracy = 0.0287\n",
      "Epoch 35/50: Train Loss = 3.4978, Train Accuracy = 0.0312\n",
      "  Validation Loss = 3.4971, Validation Accuracy = 0.0315\n",
      "Epoch 36/50: Train Loss = 3.4977, Train Accuracy = 0.0314\n",
      "  Validation Loss = 3.4969, Validation Accuracy = 0.0333\n",
      "Epoch 37/50: Train Loss = 3.4981, Train Accuracy = 0.0304\n",
      "  Validation Loss = 3.4974, Validation Accuracy = 0.0287\n",
      "Epoch 38/50: Train Loss = 3.4976, Train Accuracy = 0.0308\n",
      "  Validation Loss = 3.5000, Validation Accuracy = 0.0292\n",
      "Epoch 39/50: Train Loss = 3.4979, Train Accuracy = 0.0301\n",
      "  Validation Loss = 3.4975, Validation Accuracy = 0.0298\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold # For K-fold cross-validation\n",
    "from sklearn.preprocessing import LabelBinarizer # For one-hot encoding\n",
    "\n",
    "# --- 1. Helper Functions for Activation and Loss ---\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, Z) # [1]\n",
    "\n",
    "def relu_prime(Z):\n",
    "    \"\"\"Derivative of ReLU activation function.\"\"\"\n",
    "    return (Z > 0).astype(float) # [1]\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Softmax activation function for the output layer.\"\"\"\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True)) # Subtract max for numerical stability\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True) # [1]\n",
    "\n",
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    \"\"\"Multiclass cross-entropy loss function.\"\"\"\n",
    "    m = Y.shape[0]  # Fixed: get number of samples correctly\n",
    "    # Avoid log(0) by clipping predictions\n",
    "    Y_hat_clipped = np.clip(Y_hat, 1e-12, 1 - 1e-12)\n",
    "    loss = -np.sum(Y * np.log(Y_hat_clipped)) / m # [1]\n",
    "    return loss\n",
    "\n",
    "def calculate_accuracy(Y, Y_hat):\n",
    "    \"\"\"Calculates classification accuracy.\"\"\"\n",
    "    predictions = np.argmax(Y_hat, axis=1)\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels) # [1]\n",
    "    return accuracy\n",
    "\n",
    "# --- 2. Adam Optimizer ---\n",
    "\n",
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Adam (Adaptive Moment Estimation) optimizer.\n",
    "    Utilizes adaptive moments for improved convergence. [1]\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate # [1]\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {} # First moment estimates\n",
    "        self.v = {} # Second moment estimates\n",
    "        self.t = 0  # Time step\n",
    "\n",
    "    def update_parameters(self, parameters, gradients):\n",
    "        self.t += 1\n",
    "        for key in parameters:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(parameters[key])\n",
    "                self.v[key] = np.zeros_like(parameters[key])\n",
    "\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * gradients[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (gradients[key] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# --- 3. Neural Network (Multilayer Perceptron) ---\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron (MLP) for multiclass classification.\n",
    "    Adapted from binary classification to handle 33 distinct classes. [1]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_layers_neurons, output_size, learning_rate=0.01, lambda_reg=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_neurons = hidden_layers_neurons # Configurable [1]\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate # Initial for Adam [1]\n",
    "        self.lambda_reg = lambda_reg # L2 Regularization strength [1]\n",
    "        self.parameters = {}\n",
    "        self.cache = {} # To store intermediate values for backpropagation\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initializes weights and biases for all layers.\"\"\"\n",
    "        layer_dims = [self.input_size] + self.hidden_layers_neurons + [self.output_size]\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            # He initialization for ReLU\n",
    "            self.parameters[f'W{l}'] = np.random.randn(layer_dims[l-1], layer_dims[l]) * np.sqrt(2. / layer_dims[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((1, layer_dims[l]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation through the network. [1]\n",
    "        X: Input data (m, input_size)\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        self.cache['A0'] = X # Store input for backprop\n",
    "\n",
    "        # Hidden layers with ReLU activation\n",
    "        for l in range(1, len(self.hidden_layers_neurons) + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            Z = np.dot(A, W) + b # Z[l] = A[l-1]W[l] + b[l][1]\n",
    "            A = relu(Z) # A[l] = g[l](Z[l]) [1]\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "\n",
    "        # Output layer with Softmax activation\n",
    "        W_out = self.parameters[f'W{len(self.hidden_layers_neurons) + 1}']\n",
    "        b_out = self.parameters[f'b{len(self.hidden_layers_neurons) + 1}']\n",
    "        Z_out = np.dot(A, W_out) + b_out # Z = A[1]W + b[1]\n",
    "        A_out = softmax(Z_out) # A = softmax(Z) [1]\n",
    "        self.cache[f'Z{len(self.hidden_layers_neurons) + 1}'] = Z_out\n",
    "        self.cache[f'A{len(self.hidden_layers_neurons) + 1}'] = A_out # Y_hat\n",
    "\n",
    "        return A_out\n",
    "\n",
    "    def backward(self, X, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute gradients. [1]\n",
    "        X: Input data\n",
    "        Y: True labels (one-hot encoded)\n",
    "        Y_hat: Predicted probabilities from forward pass\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Fixed: get number of samples correctly\n",
    "        gradients = {}\n",
    "        num_layers = len(self.hidden_layers_neurons) + 1 # Total layers (hidden + output)\n",
    "\n",
    "        # Gradient for output layer (L=3)\n",
    "        dZ = Y_hat - Y # dJ/dZ = Y_hat - Y [1]\n",
    "        dW = np.dot(self.cache[f'A{num_layers-1}'].T, dZ) / m # dJ/dW[1]\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m # dJ/db[1]\n",
    "\n",
    "        # Add L2 regularization term to weight gradients [1]\n",
    "        dW += (self.lambda_reg / m) * self.parameters[f'W{num_layers}']\n",
    "\n",
    "        gradients[f'dW{num_layers}'] = dW\n",
    "        gradients[f'db{num_layers}'] = db\n",
    "\n",
    "        # Gradients for hidden layers (L=2, 1)\n",
    "        for l in reversed(range(1, num_layers)):\n",
    "            dZ_prev = np.dot(dZ, self.parameters[f'W{l+1}'].T) # (dJ/dZ[l+1] * W[l+1].T) [1]\n",
    "            dZ = dZ_prev * relu_prime(self.cache[f'Z{l}']) # dJ/dZ[l] =... * ReLU'(Z[l]) [1]\n",
    "\n",
    "            dW = np.dot(self.cache[f'A{l-1}'].T, dZ) / m # dJ/dW[l][1]\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m # dJ/db[l][1]\n",
    "\n",
    "            # Add L2 regularization term to weight gradients [1]\n",
    "            dW += (self.lambda_reg / m) * self.parameters[f'W{l}']\n",
    "\n",
    "            gradients[f'dW{l}'] = dW\n",
    "            gradients[f'db{l}'] = db\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self, gradients, optimizer):\n",
    "        \"\"\"\n",
    "        Updates network parameters using the Adam optimizer. [1]\n",
    "        \"\"\"\n",
    "        # Prepare parameters and gradients in a format suitable for AdamOptimizer\n",
    "        params_to_update = {}\n",
    "        grads_for_optimizer = {}\n",
    "        \n",
    "        # Add weights and biases to the parameters dictionary\n",
    "        for l in range(1, len(self.hidden_layers_neurons) + 2):\n",
    "            params_to_update[f'W{l}'] = self.parameters[f'W{l}']\n",
    "            params_to_update[f'b{l}'] = self.parameters[f'b{l}']\n",
    "            grads_for_optimizer[f'W{l}'] = gradients[f'dW{l}']\n",
    "            grads_for_optimizer[f'b{l}'] = gradients[f'db{l}']\n",
    "\n",
    "        optimizer.update_parameters(params_to_update, grads_for_optimizer)\n",
    "\n",
    "        # Copy updated parameters back to self.parameters\n",
    "        for key in params_to_update:\n",
    "            self.parameters[key] = params_to_update[key]\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val=None, Y_val=None, epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Trains the neural network.\n",
    "        X_train: Training features\n",
    "        Y_train: Training labels (one-hot encoded)\n",
    "        X_val: Validation features (optional)\n",
    "        Y_val: Validation labels (optional)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Size of mini-batches\n",
    "        \"\"\"\n",
    "        m = X_train.shape[0]  # Fixed: get number of samples correctly\n",
    "        optimizer = AdamOptimizer(learning_rate=self.learning_rate) # [1]\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Shuffle data for each epoch\n",
    "            permutation = np.random.permutation(m)\n",
    "            shuffled_X = X_train[permutation, :]\n",
    "            shuffled_Y = Y_train[permutation, :]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = shuffled_X[i:i + batch_size, :]\n",
    "                Y_batch = shuffled_Y[i:i + batch_size, :]\n",
    "\n",
    "                # Forward pass\n",
    "                Y_hat = self.forward(X_batch)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = cross_entropy_loss(Y_batch, Y_hat)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backpropagation\n",
    "                gradients = self.backward(X_batch, Y_batch, Y_hat)\n",
    "\n",
    "                # Update parameters\n",
    "                self.update_parameters(gradients, optimizer)\n",
    "\n",
    "                # Calculate accuracy for batch\n",
    "                batch_accuracy = calculate_accuracy(Y_batch, Y_hat)\n",
    "                epoch_accuracy += batch_accuracy\n",
    "                num_batches += 1\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            avg_epoch_accuracy = epoch_accuracy / num_batches\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs}: Train Loss = {avg_epoch_loss:.4f}, Train Accuracy = {avg_epoch_accuracy:.4f}\")\n",
    "\n",
    "            if X_val is not None and Y_val is not None:\n",
    "                val_Y_hat = self.forward(X_val)\n",
    "                val_loss = cross_entropy_loss(Y_val, val_Y_hat)\n",
    "                val_accuracy = calculate_accuracy(Y_val, val_Y_hat)\n",
    "                print(f\"  Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "# --- 4. Data Handling (Simulated/Placeholder) ---\n",
    "\n",
    "def load_and_preprocess_amhcd_data(num_samples=25740, num_classes=33, img_size=32):\n",
    "    \"\"\"\n",
    "    Placeholder function to simulate loading and preprocessing AMHCD data.\n",
    "    In a real scenario, this would load actual images, resize, convert to grayscale,\n",
    "    flatten, and normalize. [1]\n",
    "\n",
    "    Returns:\n",
    "        X (np.array): Simulated image data (num_samples, 1024 features)\n",
    "        y (np.array): Simulated labels (num_samples,)\n",
    "    \"\"\"\n",
    "    print(\"Simulating AMHCD data loading and preprocessing...\")\n",
    "    # AMHCD: 25,740 images, 33 characters, 780 images per character, 60 transcribers. [1]\n",
    "    # Images resized to 32x32 pixels and flattened to 1024 features (implying grayscale). [1]\n",
    "\n",
    "    # Simulate 1024 features (32*32 for grayscale)\n",
    "    X = np.random.rand(num_samples, img_size * img_size)\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X = X / np.max(X)\n",
    "\n",
    "    # Simulate labels (0 to 32 for 33 classes)\n",
    "    y = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "    print(f\"Simulated data shape: X={X.shape}, y={y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def augment_data(X, y, max_rotation_angle=15, max_translation_pixels=2):\n",
    "    \"\"\"\n",
    "    Placeholder for data augmentation (rotations and translations). [1]\n",
    "    In a real implementation, this would apply transformations to images.\n",
    "    For this conceptual code, we'll just duplicate and add noise to simulate augmentation.\n",
    "    \"\"\"\n",
    "    print(f\"Applying data augmentation (simulated)...\")\n",
    "    # Simulate adding rotated/translated versions by adding noise\n",
    "    X_augmented = np.vstack([X, X + np.random.normal(0, 0.05, X.shape)])\n",
    "    y_augmented = np.hstack([y, y])\n",
    "    print(f\"Augmented data shape: X_augmented={X_augmented.shape}, y_augmented={y_augmented.shape}\")\n",
    "    return X_augmented, y_augmented\n",
    "\n",
    "# --- 5. Main Execution with K-fold Cross-Validation ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters [1]\n",
    "    INPUT_SIZE = 1024 # 32x32 grayscale image flattened\n",
    "    # Hidden layers are configurable, starting with 64 and 32 neurons. [1]\n",
    "    HIDDEN_LAYERS_NEURONS = [64, 32]  # Fixed: added missing values\n",
    "    OUTPUT_SIZE = 33 # 33 TifinaghIRCAM characters [1]\n",
    "    LEARNING_RATE = 0.01 # [1]\n",
    "    LAMBDA_REG = 0.001 # L2 regularization strength [1]\n",
    "    EPOCHS = 50 # Example value, should be tuned [1]\n",
    "    BATCH_SIZE = 64 # Example value, should be tuned [1]\n",
    "    K_FOLDS = 5 # K for K-fold cross-validation [1]\n",
    "\n",
    "    # Load and preprocess simulated AMHCD data\n",
    "    X_raw, y_raw = load_and_preprocess_amhcd_data()\n",
    "\n",
    "    # Apply data augmentation [1]\n",
    "    X_augmented, y_augmented = augment_data(X_raw, y_raw)\n",
    "\n",
    "    # One-hot encode labels\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    Y_one_hot = label_binarizer.fit_transform(y_augmented)\n",
    "\n",
    "    # K-fold Cross-Validation [1]\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []  # Fixed: initialized empty list\n",
    "    fold_losses = []      # Fixed: initialized empty list\n",
    "\n",
    "    print(f\"\\nStarting {K_FOLDS}-fold cross-validation...\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_augmented)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{K_FOLDS} ---\")\n",
    "        X_train, X_val = X_augmented[train_index], X_augmented[val_index]\n",
    "        Y_train, Y_val = Y_one_hot[train_index], Y_one_hot[val_index]\n",
    "\n",
    "        # Initialize a new model for each fold\n",
    "        model = NeuralNetwork(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_layers_neurons=HIDDEN_LAYERS_NEURONS,\n",
    "            output_size=OUTPUT_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            lambda_reg=LAMBDA_REG\n",
    "        )\n",
    "\n",
    "        # Train the model for the current fold\n",
    "        model.train(X_train, Y_train, X_val, Y_val, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Evaluate final performance on validation set for this fold\n",
    "        final_val_Y_hat = model.forward(X_val)\n",
    "        final_val_loss = cross_entropy_loss(Y_val, final_val_Y_hat)\n",
    "        final_val_accuracy = calculate_accuracy(Y_val, final_val_Y_hat)\n",
    "\n",
    "        print(f\"Fold {fold + 1} Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"Fold {fold + 1} Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "\n",
    "        fold_losses.append(final_val_loss)\n",
    "        fold_accuracies.append(final_val_accuracy)\n",
    "\n",
    "    # Report average K-fold results\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    std_accuracy = np.std(fold_accuracies)\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "\n",
    "    print(\"\\n--- K-fold Cross-Validation Results ---\")\n",
    "    print(f\"Average Validation Accuracy: {avg_accuracy:.4f} +/- {std_accuracy:.4f}\")\n",
    "    print(f\"Average Validation Loss: {avg_loss:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"\\nThis code provides a conceptual implementation based on the report's requirements.\")\n",
    "    print(\"For a full, runnable solution, you would need to:\")\n",
    "    print(\"1. Download the AMHCD dataset from the specified Kaggle link.[1]\")\n",
    "    print(\"2. Implement actual image loading, resizing, grayscale conversion, and flattening.\")\n",
    "    print(\"3. Implement robust data augmentation (e.g., using scikit-image or OpenCV).\")\n",
    "    print(\"4. Potentially use a deep learning framework (TensorFlow/PyTorch) for performance.\")\n",
    "    print(\"5. Conduct thorough hyperparameter tuning for hidden layer neurons, learning rate, lambda_reg, epochs, and batch size to achieve the 'best score'.[1]\")\n",
    "    print(\"6. Address computational resource difficulties, possibly by using GPUs if available.[1]\")\n",
    "    print(\"7. Provide a public GitHub repository link in the final report.[1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71d209-e4cc-4cd3-9667-cfc7baae9e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-openCV]",
   "language": "python",
   "name": "conda-env-.conda-openCV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
